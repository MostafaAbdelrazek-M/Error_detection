{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51047,
     "status": "ok",
     "timestamp": 1705327456117,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "chU19HeC4Som",
    "outputId": "042ed109-7f6b-4e95-b668-486014a9d4e4"
   },
   "outputs": [],
   "source": [
    "# !pip install qalsadi\n",
    "# !pip install transformers\n",
    "# !pip3 install pickle5\n",
    "# !pip install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 39347,
     "status": "ok",
     "timestamp": 1705327500233,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "OmRhBn88cxAr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tatas\\anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "arabert='checkpoint-161000'\n",
    "model = AutoModelForTokenClassification.from_pretrained(arabert, num_labels=5).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(arabert)\n",
    "error_classification_arabert = pipeline(task=\"token-classification\", model=model, tokenizer=tokenizer,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1705327546550,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "vlT5JVvFh1Pq",
    "outputId": "c9a2d0df-c0d1-4306-841c-3b45b604496d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tatas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from pyarabic import araby\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('arabic')\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'«»'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "special_words=sw+list(punctuations_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1705327549461,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "JaAvORrgbglx"
   },
   "outputs": [],
   "source": [
    "labels_map={\n",
    "  'correct' : 0,\n",
    "'spelling-error':1,\n",
    "'grammer-error':2,\n",
    " 'context-error':3,\n",
    " 'seg-error':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1705327552420,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "Svpro4og4U1J"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import qalsadi.lemmatizer\n",
    "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "import pandas as pd\n",
    "with open('all_vocab_clean (1).pickle', 'rb') as fh:\n",
    "    lemma_vocab= pickle.load(fh)\n",
    "omar_vocab=list(pd.read_csv('omar_vocab (1).csv')['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1170,
     "status": "ok",
     "timestamp": 1705327558805,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "F97Ht0S0dvn-"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('omar_vocab (1).csv')\n",
    "df=df[df['freq'] > 15]\n",
    "omar_vocab=list(df['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OROEQqjxiHfw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ويبستر</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>يضيعه</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>أخوتي</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ستنتهي</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>أطيافها</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477792</th>\n",
       "      <td>يمكنهم</td>\n",
       "      <td>2426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477794</th>\n",
       "      <td>استوقفت</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477795</th>\n",
       "      <td>شقيقتها</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477796</th>\n",
       "      <td>بالمدرجات</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477798</th>\n",
       "      <td>فريدة</td>\n",
       "      <td>2331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168703 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  freq\n",
       "6          ويبستر    33\n",
       "7           يضيعه    21\n",
       "10          أخوتي    16\n",
       "11         ستنتهي   446\n",
       "14        أطيافها    43\n",
       "...           ...   ...\n",
       "477792     يمكنهم  2426\n",
       "477794    استوقفت    18\n",
       "477795    شقيقتها   350\n",
       "477796  بالمدرجات    19\n",
       "477798      فريدة  2331\n",
       "\n",
       "[168703 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1705327565086,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "ZF7kOSmKfVfv",
    "outputId": "5e02a9cf-b91b-45a6-c5f4-537f5bcf3646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168703"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(omar_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1705327567903,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "RQJ1a2V6nCVC"
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ه\", \"ة\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_txt(txt):\n",
    "    #deal with punct\n",
    "    txt=''.join(['' if char in punctuations_list  else char for char in txt])\n",
    "    #add space before and after english and numbers\n",
    "    pattern=r'([0-9]+|[\\u0660-\\u0669]+|[a-zA-Z]+)'\n",
    "    txt= re.sub(pattern,r' \\1 ',txt)\n",
    "    txt =araby.strip_tashkeel(txt)\n",
    "    #remove multiple spaces\n",
    "    txt = re.sub(' +', ' ', txt).strip()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1705327570985,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "KjOOmBmN0uB9"
   },
   "outputs": [],
   "source": [
    "def is_number_or_en_letters(token):\n",
    "    pattern=r'([0-9]+|[\\u0660-\\u0669]+|[a-zA-Z]+)'\n",
    "    if re.match(pattern,token):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1705327577003,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "VvM3ORF8fCvR"
   },
   "outputs": [],
   "source": [
    "def get_sent_tokens_labels_scores(text,do_vocab_check=True):\n",
    "    text= preprocess_txt(text)\n",
    "    results=error_classification_arabert(text)\n",
    "    labels=[]\n",
    "    scores=[]\n",
    "    words=[]\n",
    "    prev_subword=''\n",
    "    label=''\n",
    "    subword=False\n",
    "    for idx, result_dict in enumerate(results):\n",
    "\n",
    "        if '##' not in result_dict['word']:\n",
    "            if subword:\n",
    "                words.append(prev_subword)\n",
    "                subword=False\n",
    "            # if complete word last word or next word does not contain ##\n",
    "            if idx ==(len(results)-1) or '##' not in results[idx+1]['word']:\n",
    "                words.append(result_dict['word'])\n",
    "            else :\n",
    "                prev_subword=result_dict['word']\n",
    "\n",
    "            labels.append(int(result_dict['entity'].split('_')[1]))\n",
    "            scores.append(float(result_dict['score']))\n",
    "\n",
    "\n",
    "        else:\n",
    "            subword = True\n",
    "            prev_subword+=result_dict['word'].replace('#','')\n",
    "            if idx == len(results) -1 :\n",
    "                words.append(prev_subword)\n",
    "\n",
    "    if do_vocab_check:\n",
    "\n",
    "        for idx , word in enumerate(words):\n",
    "            if is_number_or_en_letters(word):\n",
    "                labels[idx] = 0\n",
    "                continue\n",
    "\n",
    "            # if label in 0 1 4 do the vocab check\n",
    "            if labels[idx]  == 2:\n",
    "                continue\n",
    "            # context and normalized word is in omar vocab\n",
    "            if labels[idx] == 3 and (normalize(word) in omar_vocab):\n",
    "                labels[idx]=0\n",
    "                continue\n",
    "\n",
    "            if (word in special_words) or  (word in omar_vocab):\n",
    "                labels[idx]=0\n",
    "                continue\n",
    "\n",
    "            #normalize the word and search again\n",
    "            normalized_word=normalize(word)\n",
    "            if  (normalized_word in special_words) or  (normalized_word in omar_vocab):\n",
    "                labels[idx]=0\n",
    "                continue\n",
    "\n",
    "            #get the lemma and search in the lemmatized vocab\n",
    "            token_lemma=lemmer.lemmatize(word)\n",
    "            if token_lemma  in lemma_vocab:\n",
    "                labels[idx] = 0\n",
    "                continue\n",
    "            #if not found in any vocab and the label is zero then assign it to be misspelt\n",
    "            if labels[idx] == 0:\n",
    "                  labels[idx]=1\n",
    "\n",
    "    return words,labels , scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2379,
     "status": "ok",
     "timestamp": 1705327583424,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "uY0IB5gFdWcH",
    "outputId": "1f652539-df18-408e-eb9e-01ab1d21fd58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['الجامعه', 'الامريكيه'], [0, 0], [0.999437153339386, 0.999747097492218])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sent_tokens_labels_scores(\"الجامعه الامريكيه\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tables import index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\ahram\\ahram\\ahram_author\\ahram_batch1.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "wrong_words_lst=[]\n",
    "wrong_words_ratio=[]\n",
    "labels_lst=[]\n",
    "ct = 0\n",
    "i = 0\n",
    "\n",
    "\n",
    "\n",
    "for title in tqdm(df[\"author\"]):\n",
    "#     try:\n",
    "    words , labels , scores= get_sent_tokens_labels_scores(str(title),do_vocab_check=True)\n",
    "    wrong_words = sum([1 if label != 0 else 0 for label in labels ])\n",
    "    if len(labels) <1 :\n",
    "        wrong_words=0\n",
    "    wrong_words_lst.append(wrong_words)\n",
    "    wrong_words_ratio.append(wrong_words /len(labels)) if len(labels) >0 else wrong_words_ratio.append(0)\n",
    "    labels_lst.append(','.join(str(label) for label in labels))\n",
    "\n",
    "    if i % 5000 == 0 and i > 0:\n",
    "        new_df2 = pd.DataFrame()\n",
    "        new_df2['wrong_words_count'] = wrong_words_lst\n",
    "        new_df2['wrong_words_ratio'] = wrong_words_ratio\n",
    "        new_df2['labels'] =labels_lst\n",
    "        new_df2.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\authors\\Authors ouutput Batch 1.csv',encoding='utf-8-sig',index=False)\n",
    "\n",
    "\n",
    "    i += 1\n",
    "#     except:\n",
    "#         ct+=1\n",
    "#         print(title)\n",
    "#         wrong_words_lst.append('nan')\n",
    "#         wrong_words_ratio.append('nan')\n",
    "#         labels_lst.append('nan')\n",
    "#         print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['wrong_words_count'] = wrong_words_lst\n",
    "new_df['wrong_words_ratio'] = wrong_words_ratio\n",
    "new_df['labels'] =labels_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\authors\\Authors ouutput Batch 1.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2469,
     "status": "ok",
     "timestamp": 1705327589543,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "QXb4ym0b0VNC",
    "outputId": "2fef27a2-20e8-47d9-c562-e041971deab2"
   },
   "outputs": [],
   "source": [
    "from tables import index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\ahram\\ahram\\ahram_persons\\persons batch 3.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tca_AgA0xhA",
    "outputId": "f2d4f68c-23e0-42c6-fd5c-379053cf6da6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "wrong_words_lst=[]\n",
    "wrong_words_ratio=[]\n",
    "labels_lst=[]\n",
    "ct = 0\n",
    "i = 0\n",
    "\n",
    "\n",
    "\n",
    "for title in tqdm(df[\"persons\"]):\n",
    "#     try:\n",
    "    words , labels , scores= get_sent_tokens_labels_scores(str(title),do_vocab_check=True)\n",
    "    wrong_words = sum([1 if label != 0 else 0 for label in labels ])\n",
    "    if len(labels) <1 :\n",
    "        wrong_words=0\n",
    "    wrong_words_lst.append(wrong_words)\n",
    "    wrong_words_ratio.append(wrong_words /len(labels)) if len(labels) >0 else wrong_words_ratio.append(0)\n",
    "    labels_lst.append(','.join(str(label) for label in labels))\n",
    "\n",
    "    if i % 5000 == 0 and i > 0:\n",
    "        new_df2 = pd.DataFrame()\n",
    "        new_df2['wrong_words_count'] = wrong_words_lst\n",
    "        new_df2['wrong_words_ratio'] = wrong_words_ratio\n",
    "        new_df2['labels'] =labels_lst\n",
    "        new_df2.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\Persons\\Persons ouutput Batch 3.csv',encoding='utf-8-sig',index=False)\n",
    "\n",
    "\n",
    "    i += 1\n",
    "#     except:\n",
    "#         ct+=1\n",
    "#         print(title)\n",
    "#         wrong_words_lst.append('nan')\n",
    "#         wrong_words_ratio.append('nan')\n",
    "#         labels_lst.append('nan')\n",
    "#         print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1703519226649,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "zrpBZJcr2ucW",
    "outputId": "bdb39634-eecc-4ea5-f1df-f4c0d6adc72b"
   },
   "outputs": [],
   "source": [
    "new_df = df\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_words_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybeqj_u1ECgo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df['wrong_words_count'] = wrong_words_lst\n",
    "new_df['wrong_words_ratio'] = wrong_words_ratio\n",
    "new_df['labels'] =labels_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703519258337,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "k7fMMkdICK0I",
    "outputId": "ea6b583a-d2fb-4bfd-aeb1-4f11b75d9f05"
   },
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGKFQdqR-zYv"
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\Persons\\Persons ouutput Batch 3.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmAY-fV7Vcgi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\ahram\\ahram\\ahram_keywords\\Keywords batch 3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1705327621883,
     "user": {
      "displayName": "Rana Reda",
      "userId": "08189578847161956537"
     },
     "user_tz": -120
    },
    "id": "Y1Lheoe4ckU0",
    "outputId": "2d20cea4-af1a-4c94-c499-ef0e9aeee3ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "wrong_words_lst=[]\n",
    "wrong_words_ratio=[]\n",
    "labels_lst=[]\n",
    "ct = 0\n",
    "i = 0\n",
    "\n",
    "\n",
    "\n",
    "for title in tqdm(df[\"keywords\"]):\n",
    "#     try:\n",
    "    words , labels , scores= get_sent_tokens_labels_scores(str(title),do_vocab_check=True)\n",
    "    wrong_words = sum([1 if label != 0 else 0 for label in labels ])\n",
    "    if len(labels) <1 :\n",
    "        wrong_words=0\n",
    "    wrong_words_lst.append(wrong_words)\n",
    "    wrong_words_ratio.append(wrong_words /len(labels)) if len(labels) >0 else wrong_words_ratio.append(0)\n",
    "    labels_lst.append(','.join(str(label) for label in labels))\n",
    "\n",
    "    if i % 5000 == 0 and i > 0:\n",
    "        new_df2 = pd.DataFrame()\n",
    "        new_df2['wrong_words_count'] = wrong_words_lst\n",
    "        new_df2['wrong_words_ratio'] = wrong_words_ratio\n",
    "        new_df2['labels'] =labels_lst\n",
    "        new_df2.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\keywords\\keywords ouutput Batch 3.csv',encoding='utf-8-sig',index=False)\n",
    "\n",
    "\n",
    "    i += 1\n",
    "#     except:\n",
    "#         ct+=1\n",
    "#         print(title)\n",
    "#         wrong_words_lst.append('nan')\n",
    "#         wrong_words_ratio.append('nan')\n",
    "#         labels_lst.append('nan')\n",
    "#         print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['wrong_words_count'] = wrong_words_lst\n",
    "new_df['wrong_words_ratio'] = wrong_words_ratio\n",
    "new_df['labels'] =labels_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\keywords\\keywords ouutput Batch 3.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\ahram\\ahram\\ahram_places\\places batch 6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 8/774189 [00:00<10:58:22, 19.60it/s]C:\\Users\\tatas\\anaconda3\\envs\\new_env\\lib\\site-packages\\transformers\\pipelines\\base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████| 774189/774189 [4:53:35<00:00, 43.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "wrong_words_lst=[]\n",
    "wrong_words_ratio=[]\n",
    "labels_lst=[]\n",
    "ct = 0\n",
    "i = 0\n",
    "\n",
    "\n",
    "\n",
    "for title in tqdm(df[\"places\"]):\n",
    "#     try:\n",
    "    words , labels , scores= get_sent_tokens_labels_scores(str(title),do_vocab_check=True)\n",
    "    wrong_words = sum([1 if label != 0 else 0 for label in labels ])\n",
    "    if len(labels) <1 :\n",
    "        wrong_words=0\n",
    "    wrong_words_lst.append(wrong_words)\n",
    "    wrong_words_ratio.append(wrong_words /len(labels)) if len(labels) >0 else wrong_words_ratio.append(0)\n",
    "    labels_lst.append(','.join(str(label) for label in labels))\n",
    "\n",
    "    if i % 5000 == 0 and i > 0:\n",
    "        new_df2 = pd.DataFrame()\n",
    "        new_df2['wrong_words_count'] = wrong_words_lst\n",
    "        new_df2['wrong_words_ratio'] = wrong_words_ratio\n",
    "        new_df2['labels'] =labels_lst\n",
    "        new_df2.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\places\\places ouutput Batch 6.csv',encoding='utf-8-sig',index=False)\n",
    "\n",
    "\n",
    "    i += 1 \n",
    "#     except:\n",
    "#         ct+=1\n",
    "#         print(title)\n",
    "#         wrong_words_lst.append('nan')\n",
    "#         wrong_words_ratio.append('nan')\n",
    "#         labels_lst.append('nan')\n",
    "#         print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocKey</th>\n",
       "      <th>keywords</th>\n",
       "      <th>persons</th>\n",
       "      <th>places</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ahram_1980_01_04_page_0013_art5</td>\n",
       "      <td>نظام الضرائب,ادم سميث,التكافل الاجتماعي,تجاره ...</td>\n",
       "      <td>المسلمين,ادم سميث,سامي رمضان</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ahram_1980_01_04_page_0013_art6</td>\n",
       "      <td>عبد الرحمن,خطبه الجمعه اليوم,جنوب شرق اسيا,الا...</td>\n",
       "      <td>المسلمين,مسلمي,الامين العام,موتمر,مايسه عبد ال...</td>\n",
       "      <td>ماليزيا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ahram_1980_01_04_page_0013_art6</td>\n",
       "      <td>عبد الرحمن,خطبه الجمعه اليوم,جنوب شرق اسيا,الا...</td>\n",
       "      <td>المسلمين,مسلمي,الامين العام,موتمر,مايسه عبد ال...</td>\n",
       "      <td>منطقه جنوب شرق اسيا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ahram_1980_01_04_page_0013_art6</td>\n",
       "      <td>عبد الرحمن,خطبه الجمعه اليوم,جنوب شرق اسيا,الا...</td>\n",
       "      <td>المسلمين,مسلمي,الامين العام,موتمر,مايسه عبد ال...</td>\n",
       "      <td>منطقه جنوب شرقي اسيا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahram_1980_01_04_page_0013_art7</td>\n",
       "      <td>مصر الجديده,ميدان الاسماعيليه</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774184</th>\n",
       "      <td>ahram_2020_10_23_page_0019_art1</td>\n",
       "      <td>قداسه البابا,دير مارمينا,القداس الالهي,دكتور م...</td>\n",
       "      <td>الانبا,البابا,نيافه,الا</td>\n",
       "      <td>بلاد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774185</th>\n",
       "      <td>ahram_2020_10_23_page_0019_art1</td>\n",
       "      <td>قداسه البابا,دير مارمينا,القداس الالهي,دكتور م...</td>\n",
       "      <td>الانبا,البابا,نيافه,الا</td>\n",
       "      <td>دير مارمينا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774186</th>\n",
       "      <td>ahram_2020_10_23_page_0019_art36</td>\n",
       "      <td>نقابه المهندسين بالغربيه,مقر النقابه,اداره الم...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>الامين</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774187</th>\n",
       "      <td>ahram_2020_10_23_page_0019_art36</td>\n",
       "      <td>نقابه المهندسين بالغربيه,مقر النقابه,اداره الم...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>النقابه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774188</th>\n",
       "      <td>ahram_2020_10_23_page_0019_art41</td>\n",
       "      <td>الملابس الجاهزه,حقوق الملكيه</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>774189 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  DocKey  \\\n",
       "0        ahram_1980_01_04_page_0013_art5   \n",
       "1        ahram_1980_01_04_page_0013_art6   \n",
       "2        ahram_1980_01_04_page_0013_art6   \n",
       "3        ahram_1980_01_04_page_0013_art6   \n",
       "4        ahram_1980_01_04_page_0013_art7   \n",
       "...                                  ...   \n",
       "774184   ahram_2020_10_23_page_0019_art1   \n",
       "774185   ahram_2020_10_23_page_0019_art1   \n",
       "774186  ahram_2020_10_23_page_0019_art36   \n",
       "774187  ahram_2020_10_23_page_0019_art36   \n",
       "774188  ahram_2020_10_23_page_0019_art41   \n",
       "\n",
       "                                                 keywords  \\\n",
       "0       نظام الضرائب,ادم سميث,التكافل الاجتماعي,تجاره ...   \n",
       "1       عبد الرحمن,خطبه الجمعه اليوم,جنوب شرق اسيا,الا...   \n",
       "2       عبد الرحمن,خطبه الجمعه اليوم,جنوب شرق اسيا,الا...   \n",
       "3       عبد الرحمن,خطبه الجمعه اليوم,جنوب شرق اسيا,الا...   \n",
       "4                           مصر الجديده,ميدان الاسماعيليه   \n",
       "...                                                   ...   \n",
       "774184  قداسه البابا,دير مارمينا,القداس الالهي,دكتور م...   \n",
       "774185  قداسه البابا,دير مارمينا,القداس الالهي,دكتور م...   \n",
       "774186  نقابه المهندسين بالغربيه,مقر النقابه,اداره الم...   \n",
       "774187  نقابه المهندسين بالغربيه,مقر النقابه,اداره الم...   \n",
       "774188                       الملابس الجاهزه,حقوق الملكيه   \n",
       "\n",
       "                                                  persons  \\\n",
       "0                            المسلمين,ادم سميث,سامي رمضان   \n",
       "1       المسلمين,مسلمي,الامين العام,موتمر,مايسه عبد ال...   \n",
       "2       المسلمين,مسلمي,الامين العام,موتمر,مايسه عبد ال...   \n",
       "3       المسلمين,مسلمي,الامين العام,موتمر,مايسه عبد ال...   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "774184                            الانبا,البابا,نيافه,الا   \n",
       "774185                            الانبا,البابا,نيافه,الا   \n",
       "774186                                                NaN   \n",
       "774187                                                NaN   \n",
       "774188                                                NaN   \n",
       "\n",
       "                      places  \n",
       "0                        NaN  \n",
       "1                    ماليزيا  \n",
       "2        منطقه جنوب شرق اسيا  \n",
       "3       منطقه جنوب شرقي اسيا  \n",
       "4                        NaN  \n",
       "...                      ...  \n",
       "774184                  بلاد  \n",
       "774185           دير مارمينا  \n",
       "774186                الامين  \n",
       "774187               النقابه  \n",
       "774188                   NaN  \n",
       "\n",
       "[774189 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['wrong_words_count'] = wrong_words_lst\n",
    "new_df['wrong_words_ratio'] = wrong_words_ratio\n",
    "new_df['labels'] =labels_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\places\\places ouutput Batch 6.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\ahram\\ahram\\ahram_author\\ahram_batch2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "wrong_words_lst=[]\n",
    "wrong_words_ratio=[]\n",
    "labels_lst=[]\n",
    "ct = 0\n",
    "i = 0\n",
    "\n",
    "\n",
    "\n",
    "for title in tqdm(df[\"author\"]):\n",
    "#     try:\n",
    "    words , labels , scores= get_sent_tokens_labels_scores(str(title),do_vocab_check=True)\n",
    "    wrong_words = sum([1 if label != 0 else 0 for label in labels ])\n",
    "    if len(labels) <1 :\n",
    "        wrong_words=0\n",
    "    wrong_words_lst.append(wrong_words)\n",
    "    wrong_words_ratio.append(wrong_words /len(labels)) if len(labels) >0 else wrong_words_ratio.append(0)\n",
    "    labels_lst.append(','.join(str(label) for label in labels))\n",
    "\n",
    "    if i % 5000 == 0 and i > 0:\n",
    "        new_df2 = pd.DataFrame()\n",
    "        new_df2['wrong_words_count'] = wrong_words_lst\n",
    "        new_df2['wrong_words_ratio'] = wrong_words_ratio\n",
    "        new_df2['labels'] =labels_lst\n",
    "        new_df2.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\authors\\authors ouutput Batch 2.csv',encoding='utf-8-sig',index=False)\n",
    "\n",
    "\n",
    "    i += 1\n",
    "#     except:\n",
    "#         ct+=1\n",
    "#         print(title)\n",
    "#         wrong_words_lst.append('nan')\n",
    "#         wrong_words_ratio.append('nan')\n",
    "#         labels_lst.append('nan')\n",
    "#         print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['wrong_words_count'] = wrong_words_lst\n",
    "new_df['wrong_words_ratio'] = wrong_words_ratio\n",
    "new_df['labels'] =labels_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(r'C:\\Users\\tatas\\Ahram Error Detection\\Output\\authors\\authors ouutput Batch 2.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Orion,Opus,Amapa,CMTL,Massar,RCMA,Merritt,Caravela Energy,Quantix,Third Street Ag,Valent,Volt,Verus,Ancora,Haidar Jupiter,Hondius,NWone,Ospraie,Quantix,Arion,GZC,Pinnacle Six One,Sarkis,Skylar,Viribus,Drakewood,Alphidence"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
